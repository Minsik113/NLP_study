● 원핫인코딩 
장점: 매우 간단, 이해가 쉽다.
단점: 1. 수가 많아지면 공간이 커지므로 비효율적이다
      2. 단순히 단어가 뭔지만 알려주고, 벡터값에 단어의 의미나 특성이 없다.
즉, 단어 벡터의 크기가 너무 크고 값이 희소(sparse), 단어 벡터가 단어의 의미나 특성을 전혀 표현할 수 없다
=>분포 가설(Distributed hypothesis)을 기반으로 표현할 수 잇는 다른 인코딩 방법들이 있다.

● 분포 가설
: 같은 문맥의 단어, 즉 비슷한 위치에 존재하는 단어는 단어 간의 유사도가 높다고 판단하는 방법으로 크게 두 가지 방법으로 나뉜다.
1. 카운트 기반 방법
: 특정 문맥 안에서 단어들이 동시에 등장하는 횟수를 직접 세는 방법
- 종류:
    특이값 분해, 잠재의미분석, Hyperspace Analogue to Language(HAL), Hellinger PCA(Principal Component Analysis)
    - 위 방법은 모두 동시 출현 행렬(Co-occurence Matrix)을 만들고 그 행렬들을 변형하는 방식이다.
- 장점: a. 적은 시간으로 단어를 만들 수 있다
        b. 데이터가 많을 경우, 단어가 잘 표현되고 효율적이어서 아직까지 많이 사용하는 방법

2. 예측 방법
: 신경망 등을 통해 문맥 안의 단어들을 예측하는 방법
- 종류:
    Word2vec NNLM(Neural Network Language Model), RNNLM(Recurrent Neural Network Language Model)
여러 예측 기반 방법 중 단어표현 방법으로 Word2vec를 가장 많이 사용한다.
Word2vec
    - CBOW(Continuous Bag of Words) 모델
    : 어떤 단어를 문맥 안의 주변 단어들을 통해 예측하는 방법
    - Skip-Gram 모델
    : 어떤 단어를 가지고 특정 문맥 안의 주변 단어들을 예측하는 방법
    
    ex) 민식은 냉장고에서 닭가슴살을 꺼내 먹었다.
    CBOW는 주변단어를 통해서 하나의 단어를 예측하는 모델이므로
    CBOW = 민식은 냉장고에서 _____을 꺼내 먹었다.
    반대로 Skip-Gram은 하나의 단어를 가지고 주변에 올 단어를 예측하는 모델이므로
    Skip-Gram = ___ _____ 음식을 ____ _____
    
    => CBOW에서는 입력값으로 여러 개의 단어를 사용하고, 학습을 위해 하나의 단어와 비교하지만,
    Skip-Gram에서는 입력값이 하나의 단어를 사용하고, 학습을 위해 주변의 여러 단어와 비교한다.
- 장점: (기존의 카운트 기반 방법보다)
    1. 단어 벡터보다 단어 간의 유사도를 잘 측정한다.
    2. 단어들의 복잡한 특징까지도 잘 잡아낸다.
    3. 이렇게 만들어진 단어 벡터는 서로에게 유의미한 관계를 측정할 수 있다.

-> 카운트 기반 방법, 예측 기반 모두 장점이 있다. 이 둘을 섞은 "Glove"라는 단어 표현 방법 또한 자주 사용된다.
